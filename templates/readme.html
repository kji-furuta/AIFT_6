{% extends "base.html" %}

{% block title %}操作マニュアル - AI Fine-tuning Toolkit{% endblock %}

{% block extra_styles %}
.readme-content {
    background: white;
    padding: 30px;
    border-radius: 10px;
    box-shadow: 0 5px 15px rgba(0,0,0,0.1);
    line-height: 1.6;
    font-size: 16px;
}

.readme-content h1 {
    color: #2c3e50;
    border-bottom: 3px solid #3498db;
    padding-bottom: 10px;
    margin-bottom: 30px;
}

.readme-content h2 {
    color: #34495e;
    border-left: 4px solid #3498db;
    padding-left: 15px;
    margin-top: 40px;
    margin-bottom: 20px;
}

.readme-content h3 {
    color: #2c3e50;
    margin-top: 30px;
    margin-bottom: 15px;
}

.readme-content p {
    margin-bottom: 15px;
    color: #2c3e50;
}

.readme-content ul, .readme-content ol {
    margin-bottom: 20px;
    padding-left: 30px;
}

.readme-content li {
    margin-bottom: 8px;
    color: #2c3e50;
}

.readme-content code {
    background: #f8f9fa;
    padding: 2px 6px;
    border-radius: 4px;
    font-family: 'Courier New', monospace;
    color: #e74c3c;
}

.readme-content pre {
    background: #2c3e50;
    color: #ecf0f1;
    padding: 20px;
    border-radius: 8px;
    overflow-x: auto;
    margin: 20px 0;
}

.readme-content pre code {
    background: none;
    color: inherit;
    padding: 0;
}

.readme-content blockquote {
    border-left: 4px solid #3498db;
    padding-left: 20px;
    margin: 20px 0;
    color: #7f8c8d;
    font-style: italic;
}

.readme-content table {
    width: 100%;
    border-collapse: collapse;
    margin: 20px 0;
}

.readme-content th, .readme-content td {
    border: 1px solid #ddd;
    padding: 12px;
    text-align: left;
}

.readme-content th {
    background: #3498db;
    color: white;
    font-weight: bold;
}

.readme-content tr:nth-child(even) {
    background: #f8f9fa;
}

.readme-content a {
    color: #3498db;
    text-decoration: none;
}

.readme-content a:hover {
    text-decoration: underline;
}

.readme-content .emoji {
    font-size: 1.2em;
    margin-right: 5px;
}

.readme-content .highlight {
    background: #fff3cd;
    padding: 15px;
    border-radius: 5px;
    border-left: 4px solid #ffc107;
    margin: 20px 0;
}

.readme-content .warning {
    background: #f8d7da;
    padding: 15px;
    border-radius: 5px;
    border-left: 4px solid #dc3545;
    margin: 20px 0;
}

.readme-content .success {
    background: #d4edda;
    padding: 15px;
    border-radius: 5px;
    border-left: 4px solid #28a745;
    margin: 20px 0;
}
{% endblock %}

{% block main_title %}
<h1>
    <img src="/static/logo_teikoku.png" alt="Logo" class="title-logo">
    📖 操作マニュアル
</h1>
<p>AI Fine-tuning Toolkit の詳細な使用方法</p>
{% endblock %}

{% block content %}
<div class="readme-content">
    <h1>AI Fine-tuning Toolkit</h1>
    
    <p><span class="emoji">🚀</span> <strong>日本語LLMファインチューニング用の統合Webツールキット</strong></p>
    
    <p>Dockerベースの統合Webインターフェースで、日本語大規模言語モデル（LLM）のファインチューニングを簡単に実行できます。フルファインチューニング、LoRA、QLoRAなど複数の手法をWebブラウザから直感的に操作可能です。</p>

    <h2><span class="emoji">🌟</span> 主要機能</h2>

    <h3><span class="emoji">🌐</span> 統合Webインターフェース</h3>
    <ul>
        <li><strong>ブラウザベースUI</strong>: http://localhost:8050 でアクセス</li>
        <li><strong>リアルタイム監視</strong>: ファインチューニング進捗の可視化</li>
        <li><strong>モデル管理</strong>: 学習済みモデルの一覧・選択・生成</li>
        <li><strong>データアップロード</strong>: JSONLファイルの簡単アップロード</li>
        <li><strong>システム情報</strong>: GPU使用状況とメモリ監視</li>
        <li><strong>プロフェッショナルデザイン</strong>: 帝国大学ロゴと洗練されたUI</li>
    </ul>

    <h3>ファインチューニング手法</h3>
    <ul>
        <li><span class="emoji">🔥</span> <strong>フルファインチューニング</strong>: 全パラメータ更新による高精度学習</li>
        <li><span class="emoji">⚡</span> <strong>LoRA</strong>: パラメータ効率的学習（低メモリ）</li>
        <li><span class="emoji">💎</span> <strong>QLoRA</strong>: 4bit/8bit量子化による超省メモリ学習</li>
        <li><span class="emoji">🧠</span> <strong>EWC</strong>: 継続的学習による破滅的忘却の抑制</li>
        <li><span class="emoji">🔧</span> <strong>自動量子化</strong>: モデルサイズに応じた最適化</li>
    </ul>

    <h2><span class="emoji">✅</span> サポートモデル</h2>
    <p>最新のサポートモデルリストです。</p>

    <table>
        <thead>
            <tr>
                <th>モデル名</th>
                <th>タイプ</th>
                <th>精度</th>
                <th>推奨VRAM</th>
                <th>タグ</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><strong>Qwen/Qwen2.5-14B-Instruct</strong></td>
                <td>CausalLM</td>
                <td>bfloat16</td>
                <td>32GB</td>
                <td><code>multilingual</code>, <code>14b</code>, <code>instruct</code></td>
            </tr>
            <tr>
                <td><strong>cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese</strong></td>
                <td>CausalLM</td>
                <td>bfloat16</td>
                <td>80GB</td>
                <td><code>japanese</code>, <code>32b</code>, <code>deepseek</code></td>
            </tr>
            <tr>
                <td><strong>cyberagent/calm3-22b-chat</strong></td>
                <td>CausalLM</td>
                <td>float16</td>
                <td>48GB</td>
                <td><code>japanese</code>, <code>22b</code>, <code>chat</code></td>
            </tr>
            <tr>
                <td><strong>meta-llama/Meta-Llama-3.1-70B-Instruct</strong></td>
                <td>CausalLM</td>
                <td>bfloat16</td>
                <td>160GB</td>
                <td><code>multilingual</code>, <code>70b</code>, <code>instruct</code></td>
            </tr>
        </tbody>
    </table>

    <h2><span class="emoji">🚀</span> クイックスタート</h2>

    <h3>1. リポジトリのクローン</h3>
    <pre><code>git clone https://github.com/kji-furuta/AI_FT_3.git
cd AI_FT_3</code></pre>

    <h3>2. Docker環境の起動</h3>
    <pre><code>cd docker
docker-compose up -d --build</code></pre>

    <h3>3. Webインターフェースの起動</h3>
    <pre><code>docker exec ai-ft-container bash /workspace/scripts/start_web_interface.sh</code></pre>

    <h3>4. ブラウザでアクセス</h3>
    <pre><code>http://localhost:8050</code></pre>

    <div class="highlight">
        <h3><span class="emoji">🎯</span> 使用可能な機能</h3>
        <ul>
            <li><strong>ダッシュボード</strong>: システム状況とタスク管理</li>
            <li><strong>ファインチューニング</strong>: データアップロードと学習実行</li>
            <li><strong>テキスト生成</strong>: 学習済みモデルでの推論</li>
            <li><strong>モデル管理</strong>: 利用可能モデルと学習済みモデル一覧</li>
            <li><strong>マニュアル</strong>: <code>/manual</code> - 詳細な利用方法</li>
            <li><strong>システム概要</strong>: <code>/system-overview</code> - 技術仕様</li>
        </ul>
    </div>

    <h2><span class="emoji">📚</span> 使用方法</h2>

    <h3><span class="emoji">🌐</span> Webインターフェース（推奨）</h3>
    <p>ブラウザで <code>http://localhost:8050</code> にアクセスして以下の機能を利用：</p>

    <h4>1. ファインチューニング</h4>
    <ol>
        <li><strong>データアップロード</strong>: JSONLファイルを選択・アップロード</li>
        <li><strong>モデル選択</strong>: 利用可能なベースモデルから選択</li>
        <li><strong>設定調整</strong>: LoRA/QLoRA/フルファインチューニングの選択</li>
        <li><strong>実行監視</strong>: リアルタイム進捗とログの確認</li>
    </ol>

    <h4>2. テキスト生成</h4>
    <ol>
        <li><strong>モデル選択</strong>: 学習済みモデルの選択</li>
        <li><strong>プロンプト入力</strong>: 生成したいテキストの入力</li>
        <li><strong>パラメータ調整</strong>: 温度、最大長などの設定</li>
        <li><strong>結果確認</strong>: 生成されたテキストの表示・保存</li>
    </ol>

    <h4>3. システム管理</h4>
    <ul>
        <li><strong>システム情報</strong>: GPU使用状況とメモリ監視</li>
        <li><strong>モデル一覧</strong>: 利用可能・学習済みモデルの管理</li>
        <li><strong>ドキュメント</strong>: マニュアルと技術仕様の参照</li>
    </ul>

    <h2><span class="emoji">🔧</span> トラブルシューティング</h2>

    <h3>ロゴが表示されない場合</h3>
    <pre><code># 静的ファイルの確認
docker exec ai-ft-container ls -la /workspace/static/

# ロゴファイルの存在確認
docker exec ai-ft-container curl -I http://localhost:8050/static/logo_teikoku.png</code></pre>

    <h3>Webインターフェースが起動しない場合</h3>
    <pre><code># コンテナの状態確認
docker ps -a

# ログの確認
docker logs ai-ft-container

# 手動起動
docker exec -d ai-ft-container python -m uvicorn app.main_unified:app --host 0.0.0.0 --port 8050 --reload</code></pre>

    <div class="success">
        <h3><span class="emoji">🎯</span> 今すぐ始める</h3>
        <pre><code># 1. クローン
git clone https://github.com/kji-furuta/AI_FT_3.git
cd AI_FT_3

# 2. 起動
cd docker && docker-compose up -d --build

# 3. Webサーバー開始
docker exec ai-ft-container bash /workspace/scripts/start_web_interface.sh

# 4. ブラウザでアクセス
# http://localhost:8050</code></pre>
        <p><strong>🚀 5分でファインチューニング開始！</strong></p>
    </div>

    <h2><span class="emoji">✨</span> 主な特徴</h2>

    <h3><span class="emoji">🎯</span> 簡単操作</h3>
    <ul>
        <li><strong>ワンクリック起動</strong>: Docker Composeで環境構築完了</li>
        <li><strong>ブラウザ操作</strong>: プログラミング不要のWebUI</li>
        <li><strong>リアルタイム監視</strong>: 学習進捗とGPU使用状況を可視化</li>
        <li><strong>自動最適化</strong>: モデルサイズに応じた量子化設定</li>
        <li><strong>プロフェッショナルUI</strong>: 帝国大学ロゴと洗練されたデザイン</li>
    </ul>

    <h3><span class="emoji">🚀</span> 高性能</h3>
    <ul>
        <li><strong>GPU最適化</strong>: CUDA 12.6 + PyTorch 2.7.1</li>
        <li><strong>メモリ効率</strong>: 動的量子化とキャッシュ管理</li>
        <li><strong>マルチモデル対応</strong>: 3B〜70Bモデルまでサポート</li>
        <li><strong>DeepSpeed対応</strong>: 将来の大規模学習に対応</li>
        <li><strong>静的ファイル最適化</strong>: 統合されたディレクトリ構造</li>
    </ul>

    <h3><span class="emoji">🎨</span> UI/UX改善</h3>
    <ul>
        <li><strong>ロゴ統合</strong>: 帝国大学ロゴ（300px × 150px）の表示</li>
        <li><strong>レスポンシブデザイン</strong>: 様々な画面サイズに対応</li>
        <li><strong>ダークテーマ</strong>: 濃い背景色と薄い文字色で視認性向上</li>
        <li><strong>コンパクトレイアウト</strong>: 効率的なスペース利用</li>
    </ul>

    <h2><span class="emoji">📄</span> ライセンス</h2>
    <p>このプロジェクトはMITライセンスの下で公開されています。</p>

    <h2><span class="emoji">🙏</span> 謝辞</h2>
    <ul>
        <li><a href="https://github.com/huggingface/transformers">Hugging Face Transformers</a></li>
        <li><a href="https://github.com/huggingface/peft">Hugging Face PEFT</a></li>
        <li><a href="https://github.com/TimDettmers/bitsandbytes">BitsAndBytes</a></li>
        <li><a href="https://github.com/huggingface/accelerate">Accelerate</a></li>
    </ul>

    <h2><span class="emoji">📚</span> 関連ドキュメント</h2>
    <ul>
        <li><a href="docs/API_REFERENCE.md">API リファレンス</a> - 詳細なAPI仕様</li>
        <li><a href="docs/LARGE_MODEL_SETUP.md">大規模モデルセットアップ</a> - 32B+モデルの設定方法</li>
        <li><a href="docs/MULTI_GPU_OPTIMIZATION.md">マルチGPU最適化</a> - 分散学習の設定</li>
    </ul>

    <h3><span class="emoji">🌐</span> Webドキュメント</h3>
    <ul>
        <li><strong>利用マニュアル</strong>: <a href="http://localhost:8050/manual">http://localhost:8050/manual</a></li>
        <li><strong>システム概要</strong>: <a href="http://localhost:8050/system-overview">http://localhost:8050/system-overview</a></li>
    </ul>
</div>
{% endblock %} 